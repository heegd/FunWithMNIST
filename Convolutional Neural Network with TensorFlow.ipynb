{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# This is just to make TensorFlow use only one of my GPUs.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "This model architecture is inspired by LeNet-5\n",
    "\n",
    "1. A convolutional layer with 6 filters, a filter size of 5x5, same padding, relu activation.\n",
    "2. A max pool layer with a size of 2x2 and a stride of 2.\n",
    "3. A convulutional layer with 16 filters, a filter size of 5x5, same padding, relu activation.\n",
    "4. A max pool layer with a size of 2x2 and a stride of 2.\n",
    "5. A Fully connected layer with 120 hidden units and relu activation.\n",
    "6. A Fully connected layer with 84 hidden units and relu activation.\n",
    "7. The output fully connected layer with 10 units and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x_tensor):\n",
    "    \n",
    "    # The first conv layer should have 6 filters, a filter size of 5x5, same padding,\n",
    "    # and the relu activation function.\n",
    "    with tf.variable_scope('Conv1'):\n",
    "        w_tensor = tf.get_variable('w', \n",
    "                                   initializer=tf.truncated_normal([5, 5, 1, 6], stddev=0.1))\n",
    "        \n",
    "        b_tensor = tf.get_variable('b', initializer=tf.constant(0.1, shape=[6]))\n",
    "        \n",
    "        conv_tensor = tf.nn.conv2d(x_tensor, \n",
    "                                   w_tensor, \n",
    "                                   strides=[1, 1, 1, 1], \n",
    "                                   padding='SAME', \n",
    "                                   name='conv')\n",
    "        \n",
    "        a_tensor = tf.nn.relu(conv_tensor + b_tensor, name='a')\n",
    "        \n",
    "    # The first pool layer should have a size of 2x2.\n",
    "    with tf.variable_scope('Pool1'):\n",
    "        pool_tensor = tf.nn.max_pool(a_tensor, \n",
    "                                     ksize=[1, 2, 2, 1], \n",
    "                                     strides=[1, 2, 2, 1], \n",
    "                                     padding='SAME', \n",
    "                                     name='pool')\n",
    "        \n",
    "    # The second conv layer should have 16 filters, a filter size of 5x5, valid padding,\n",
    "    # and the relu activation function.\n",
    "    with tf.variable_scope('Conv2'):\n",
    "        w_tensor = tf.get_variable('w', \n",
    "                                   initializer=tf.truncated_normal([5, 5, 6, 16], stddev=0.1))\n",
    "        \n",
    "        b_tensor = tf.get_variable('b', initializer=tf.constant(0.1, shape=[16]))\n",
    "        \n",
    "        conv_tensor = tf.nn.conv2d(pool_tensor, \n",
    "                                   w_tensor, \n",
    "                                   strides=[1, 1, 1, 1], \n",
    "                                   padding='SAME', \n",
    "                                   name='conv')\n",
    "        \n",
    "        a_tensor = tf.nn.relu(conv_tensor + b_tensor, name='a')\n",
    "        \n",
    "    # The second pool layer should have a size of 2x2.\n",
    "    with tf.variable_scope('Pool2'):\n",
    "        pool_tensor = tf.nn.max_pool(a_tensor, \n",
    "                                     ksize=[1, 2, 2, 1], \n",
    "                                     strides=[1, 2, 2, 1], \n",
    "                                     padding='SAME', \n",
    "                                     name='pool')\n",
    "        \n",
    "    # Flatten the tensor in preparation for the fully connected layer.\n",
    "    # We started with 28x28, but each pool layer halved the dimensions.\n",
    "    # Also, we ended with 16 filters, so the flattened tensor should\n",
    "    # have a shape of (n, 7*7*16)\n",
    "    with tf.variable_scope('Flatten'):\n",
    "        flatten_tensor = tf.reshape(pool_tensor, [-1, 7*7*16], name='flatten')\n",
    "        \n",
    "    # 120 hidden units in the first layer.\n",
    "    with tf.variable_scope('Fc1'):\n",
    "        w_tensor = tf.get_variable('w', initializer=tf.truncated_normal([7*7*16, 120], stddev=0.1))\n",
    "        b_tensor = tf.get_variable('b', initializer=tf.constant(0.1, shape=[120]))\n",
    "        a_tensor = tf.nn.relu(tf.matmul(flatten_tensor, w_tensor) + b_tensor, name='a')\n",
    "    \n",
    "    # 84 hidden units in the second layer.\n",
    "    with tf.variable_scope('Fc2'):\n",
    "        w_tensor = tf.get_variable('w', initializer=tf.truncated_normal([120, 10], stddev=0.1))\n",
    "        b_tensor = tf.get_variable('b', initializer=tf.constant(0.1, shape=[10]))\n",
    "        z_tensor = tf.matmul(a_tensor, w_tensor) + b_tensor\n",
    "   \n",
    "    return z_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Cost\n",
    "The measurement of how how well the parameters fit the training values during training.  The goal is the minimize this difference.\n",
    "\n",
    "\n",
    "In TensorFlow, the last activation function is built into the cost.  Here, we are using the softmax activation which gives a probability of each output class being true.  All the probabilities sum to 1 for each example.\n",
    "\n",
    "\n",
    "The cost function we are using is cross entropy, which measures the distance between the tensor of probabilities from the output of the softmax and the actual values, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cost(z_tensor, y_tensor):\n",
    "    with tf.variable_scope('CostFunction'):\n",
    "        cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=z_tensor, labels=y_tensor)\n",
    "        cost_tensor = tf.reduce_mean(cost_tensor)\n",
    "        \n",
    "    return cost_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimizer\n",
    "Chose which optimization algorithm to use.  This is the algorithm that adjusts the weights and biases each execution to bring the cost down.  TensorFlow comes with a good selection of pre-built optimizers.  We'll use Adam here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(cost_tensor, learning_rate):\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer_tensor = tf.train.AdamOptimizer(learning_rate).minimize(cost_tensor)\n",
    "        \n",
    "    return optimizer_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Accuracy Measurement\n",
    "The measurement of accurate the model is at predicting outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_accuracy(y_hat_tensor, y_tensor):\n",
    "    with tf.variable_scope('AccuracyFunction'):\n",
    "        # A bool tensor of where the predictions matched the labels.\n",
    "        correct_predictions_tensor = tf.equal(tf.argmax(y_hat_tensor, axis=1), \n",
    "                                              tf.argmax(y_tensor, axis=1))\n",
    "        \n",
    "        # Convert the true/false values into 0 or 1.\n",
    "        correct_predictions_tensor = tf.cast(correct_predictions_tensor, tf.float32)\n",
    "        \n",
    "        # The mean of the correct_preditions_tensor will now give us the accuracy.\n",
    "        accuracy_tensor = tf.reduce_mean(correct_predictions_tensor)\n",
    "        \n",
    "    return accuracy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "Put the pieces together to build the model, input features, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(learning_rate=0.0001, iterations=1000, batch_size=100):\n",
    "    ops.reset_default_graph()\n",
    "    \n",
    "    x_tensor = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    y_tensor = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    z_tensor = build_model(x_tensor)\n",
    "    cost_tensor = build_cost(z_tensor, y_tensor)\n",
    "    optimizer_tensor = build_optimizer(cost_tensor, learning_rate)\n",
    "    accuracy_tensor = build_accuracy(z_tensor, y_tensor)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(1, iterations + 1):\n",
    "            x, y = mnist_data.train.next_batch(batch_size)\n",
    "            x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "            accuracy, cost, _ = sess.run([accuracy_tensor, cost_tensor, optimizer_tensor], \n",
    "                                         feed_dict={x_tensor:x, y_tensor:y})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print('Iteration {0} cost: {1}, accuracy: {2}'.format(i, cost, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!\n",
    "With a convolutional neural network the train accuracy is often 100% for batches.  Keep in mind that test accuracy will be a bit lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 cost: 0.15039171278476715, accuracy: 0.949999988079071\n",
      "Iteration 200 cost: 0.14551039040088654, accuracy: 0.9399999976158142\n",
      "Iteration 300 cost: 0.08272869884967804, accuracy: 0.9700000286102295\n",
      "Iteration 400 cost: 0.11444041132926941, accuracy: 0.9599999785423279\n",
      "Iteration 500 cost: 0.07795067876577377, accuracy: 0.9599999785423279\n",
      "Iteration 600 cost: 0.06593916565179825, accuracy: 0.9700000286102295\n",
      "Iteration 700 cost: 0.025510864332318306, accuracy: 0.9900000095367432\n",
      "Iteration 800 cost: 0.0504082553088665, accuracy: 0.9800000190734863\n",
      "Iteration 900 cost: 0.07934822887182236, accuracy: 0.9800000190734863\n",
      "Iteration 1000 cost: 0.061871692538261414, accuracy: 0.9900000095367432\n",
      "Iteration 1100 cost: 0.06026923656463623, accuracy: 0.9800000190734863\n",
      "Iteration 1200 cost: 0.056168485432863235, accuracy: 0.9700000286102295\n",
      "Iteration 1300 cost: 0.09596701711416245, accuracy: 0.9900000095367432\n",
      "Iteration 1400 cost: 0.012660035863518715, accuracy: 1.0\n",
      "Iteration 1500 cost: 0.027089521288871765, accuracy: 0.9800000190734863\n",
      "Iteration 1600 cost: 0.12605901062488556, accuracy: 0.9599999785423279\n",
      "Iteration 1700 cost: 0.06982661783695221, accuracy: 0.9900000095367432\n",
      "Iteration 1800 cost: 0.09467059373855591, accuracy: 0.9800000190734863\n",
      "Iteration 1900 cost: 0.059350062161684036, accuracy: 0.9800000190734863\n",
      "Iteration 2000 cost: 0.05819856747984886, accuracy: 0.9700000286102295\n",
      "Iteration 2100 cost: 0.029000043869018555, accuracy: 0.9900000095367432\n",
      "Iteration 2200 cost: 0.03885607793927193, accuracy: 0.9800000190734863\n",
      "Iteration 2300 cost: 0.13735182583332062, accuracy: 0.9800000190734863\n",
      "Iteration 2400 cost: 0.04672551527619362, accuracy: 0.9800000190734863\n",
      "Iteration 2500 cost: 0.011456932872533798, accuracy: 1.0\n",
      "Iteration 2600 cost: 0.06187106668949127, accuracy: 0.9900000095367432\n",
      "Iteration 2700 cost: 0.014365112408995628, accuracy: 1.0\n",
      "Iteration 2800 cost: 0.0523827038705349, accuracy: 0.9700000286102295\n",
      "Iteration 2900 cost: 0.023759178817272186, accuracy: 0.9800000190734863\n",
      "Iteration 3000 cost: 0.012132790870964527, accuracy: 1.0\n",
      "Iteration 3100 cost: 0.07388193160295486, accuracy: 0.9700000286102295\n",
      "Iteration 3200 cost: 0.10959530621767044, accuracy: 0.9700000286102295\n",
      "Iteration 3300 cost: 0.02119726501405239, accuracy: 0.9800000190734863\n",
      "Iteration 3400 cost: 0.011772239580750465, accuracy: 1.0\n",
      "Iteration 3500 cost: 0.07209080457687378, accuracy: 0.9800000190734863\n",
      "Iteration 3600 cost: 0.014913732185959816, accuracy: 1.0\n",
      "Iteration 3700 cost: 0.04412055015563965, accuracy: 0.9800000190734863\n",
      "Iteration 3800 cost: 0.11920704692602158, accuracy: 0.9800000190734863\n",
      "Iteration 3900 cost: 0.06547026336193085, accuracy: 0.9800000190734863\n",
      "Iteration 4000 cost: 0.07107934355735779, accuracy: 0.9700000286102295\n",
      "Iteration 4100 cost: 0.02698586881160736, accuracy: 0.9900000095367432\n",
      "Iteration 4200 cost: 0.01573983021080494, accuracy: 0.9900000095367432\n",
      "Iteration 4300 cost: 0.03325967118144035, accuracy: 0.9900000095367432\n",
      "Iteration 4400 cost: 0.04089842364192009, accuracy: 0.9900000095367432\n",
      "Iteration 4500 cost: 0.00817818008363247, accuracy: 1.0\n",
      "Iteration 4600 cost: 0.0013563265092670918, accuracy: 1.0\n",
      "Iteration 4700 cost: 0.07596811652183533, accuracy: 0.9800000190734863\n",
      "Iteration 4800 cost: 0.002662158804014325, accuracy: 1.0\n",
      "Iteration 4900 cost: 0.028103573247790337, accuracy: 0.9900000095367432\n",
      "Iteration 5000 cost: 0.05552757903933525, accuracy: 0.9900000095367432\n",
      "CPU times: user 17.8 s, sys: 2.42 s, total: 20.2 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(learning_rate = 0.01, iterations=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
