{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "**For Logistic Regression recall that:**  \n",
    "A linear function is first calculated.  \n",
    "**z = xw + b**  \n",
    "Where **w** is a tensor of weights, **x** is a tensor of features, and **b** is a tensor of biases.  \n",
    "\n",
    "\n",
    "Then the output of the linear function is run through an activation function.  \n",
    "**a = g(z)**  \n",
    "Where **g()** is the activation function.\n",
    "\n",
    "\n",
    "With TensorFlow the activation of the last linear function is built in to the cost function.  So all we have to implement here is the linear function that calculates z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x_tensor, num_features, num_labels):\n",
    "    # Initialize tensors for weights and biases.\n",
    "    with tf.variable_scope('ParameterInitialization', tf.AUTO_REUSE):\n",
    "        w_tensor = tf.get_variable(name='w', shape=(num_features, num_labels), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_tensor = tf.get_variable(name='b', shape=(1, num_labels), initializer=tf.zeros_initializer())\n",
    "        \n",
    "    # The linear function\n",
    "    with tf.variable_scope('LinearFunction', tf.AUTO_REUSE):\n",
    "        z_tensor = tf.matmul(x_tensor, w_tensor) + b_tensor\n",
    "        \n",
    "    return z_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Cost\n",
    "The measurement of how how well the parameters fit the training values during training.  The goal is the minimize this difference.\n",
    "\n",
    "\n",
    "In TensorFlow, the last activation function is built into the cost.  Here, we are using the softmax activation which gives a probability of each output class being true.  All the probabilities sum to 1 for each example.\n",
    "\n",
    "\n",
    "The cost function we are using is cross entropy, which measures the distance between the tensor of probabilities from the output of the softmax and the actual values, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cost(z_tensor, y_tensor):\n",
    "    with tf.variable_scope('CostFunction'):\n",
    "        cost_tensor = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z_tensor, labels=y_tensor))\n",
    "        \n",
    "    return cost_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimizer\n",
    "Chose which optimization algorithm to use.  This is the algorithm that adjusts the weights and biases each execution to bring the cost down.  TensorFlow comes with a good selection of pre-built optimizers.  We'll use Adam here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(cost_tensor, learning_rate):\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer_tensor = tf.train.AdamOptimizer(learning_rate).minimize(cost_tensor)\n",
    "        \n",
    "    return optimizer_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Accuracy Measurement\n",
    "The measurement of accurate the model is at predicting outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_accuracy(y_hat_tensor, y_tensor):\n",
    "    with tf.variable_scope('AccuracyFunction'):\n",
    "        # A bool tensor of where the predictions matched the labels.\n",
    "        correct_predictions_tensor = tf.equal(tf.argmax(y_hat_tensor, axis=1), tf.argmax(y_tensor, axis=1))\n",
    "        # Convert the true/false values into 0 or 1.\n",
    "        correct_predictions_tensor = tf.cast(correct_predictions_tensor, tf.float32)\n",
    "        # The mean of the correct_preditions_tensor will now give us the accuracy.\n",
    "        accuracy_tensor = tf.reduce_mean(correct_predictions_tensor)\n",
    "        \n",
    "    return accuracy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "Put the pieces together to build the model, input features, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learning_rate=0.0001, iterations=1000, batch_size=100):\n",
    "    ops.reset_default_graph()\n",
    "    \n",
    "    x_tensor = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_tensor = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    z_tensor = build_model(x_tensor, 784, 10)\n",
    "    cost_tensor = build_cost(z_tensor, y_tensor)\n",
    "    optimizer_tensor = build_optimizer(cost_tensor, learning_rate)\n",
    "    accuracy_tensor = build_accuracy(z_tensor, y_tensor)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(1, iterations + 1):\n",
    "            x, y = mnist_data.train.next_batch(batch_size)\n",
    "            accuracy, cost, _ = sess.run([accuracy_tensor, cost_tensor, optimizer_tensor], feed_dict={x_tensor:x, y_tensor:y})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print('Iteration {0} cost: {1}, accuracy: {2}'.format(i, cost, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!\n",
    "With logistic regression the train accuracy is in the low 90's for the MNIST dataset.  More complex models can get much better accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 cost: 0.2837828993797302, accuracy: 0.9110000133514404\n",
      "Iteration 200 cost: 0.2891358733177185, accuracy: 0.9139999747276306\n",
      "Iteration 300 cost: 0.2710944712162018, accuracy: 0.9259999990463257\n",
      "Iteration 400 cost: 0.2753347158432007, accuracy: 0.9269999861717224\n",
      "Iteration 500 cost: 0.3115732967853546, accuracy: 0.9179999828338623\n",
      "Iteration 600 cost: 0.272449791431427, accuracy: 0.9269999861717224\n",
      "Iteration 700 cost: 0.25753068923950195, accuracy: 0.9290000200271606\n",
      "Iteration 800 cost: 0.25930750370025635, accuracy: 0.9359999895095825\n",
      "Iteration 900 cost: 0.28241488337516785, accuracy: 0.9169999957084656\n",
      "Iteration 1000 cost: 0.26693710684776306, accuracy: 0.9269999861717224\n",
      "Iteration 1100 cost: 0.2183326929807663, accuracy: 0.9459999799728394\n",
      "Iteration 1200 cost: 0.2202308624982834, accuracy: 0.9390000104904175\n",
      "Iteration 1300 cost: 0.21753156185150146, accuracy: 0.9380000233650208\n",
      "Iteration 1400 cost: 0.24249324202537537, accuracy: 0.9359999895095825\n",
      "Iteration 1500 cost: 0.21276317536830902, accuracy: 0.9419999718666077\n",
      "Iteration 1600 cost: 0.1642550379037857, accuracy: 0.9480000138282776\n",
      "Iteration 1700 cost: 0.2573399245738983, accuracy: 0.9350000023841858\n",
      "Iteration 1800 cost: 0.2069600224494934, accuracy: 0.9419999718666077\n",
      "Iteration 1900 cost: 0.2660093903541565, accuracy: 0.9319999814033508\n",
      "Iteration 2000 cost: 0.19224175810813904, accuracy: 0.9440000057220459\n",
      "Iteration 2100 cost: 0.25203558802604675, accuracy: 0.9419999718666077\n",
      "Iteration 2200 cost: 0.2111550122499466, accuracy: 0.9440000057220459\n",
      "Iteration 2300 cost: 0.26192742586135864, accuracy: 0.9229999780654907\n",
      "Iteration 2400 cost: 0.21659798920154572, accuracy: 0.9300000071525574\n",
      "Iteration 2500 cost: 0.20819805562496185, accuracy: 0.9470000267028809\n",
      "Iteration 2600 cost: 0.23516906797885895, accuracy: 0.9359999895095825\n",
      "Iteration 2700 cost: 0.26370003819465637, accuracy: 0.9330000281333923\n",
      "Iteration 2800 cost: 0.23878665268421173, accuracy: 0.9330000281333923\n",
      "Iteration 2900 cost: 0.20331324636936188, accuracy: 0.9459999799728394\n",
      "Iteration 3000 cost: 0.19534514844417572, accuracy: 0.9409999847412109\n",
      "Iteration 3100 cost: 0.26845651865005493, accuracy: 0.9279999732971191\n",
      "Iteration 3200 cost: 0.21589095890522003, accuracy: 0.9430000185966492\n",
      "Iteration 3300 cost: 0.243624746799469, accuracy: 0.9330000281333923\n",
      "Iteration 3400 cost: 0.2808151841163635, accuracy: 0.9290000200271606\n",
      "Iteration 3500 cost: 0.20923538506031036, accuracy: 0.9430000185966492\n",
      "Iteration 3600 cost: 0.24160432815551758, accuracy: 0.9309999942779541\n",
      "Iteration 3700 cost: 0.2089638113975525, accuracy: 0.9369999766349792\n",
      "Iteration 3800 cost: 0.23009295761585236, accuracy: 0.9330000281333923\n",
      "Iteration 3900 cost: 0.21822310984134674, accuracy: 0.9390000104904175\n",
      "Iteration 4000 cost: 0.24143774807453156, accuracy: 0.9279999732971191\n",
      "Iteration 4100 cost: 0.21648362278938293, accuracy: 0.9440000057220459\n",
      "Iteration 4200 cost: 0.20401030778884888, accuracy: 0.9300000071525574\n",
      "Iteration 4300 cost: 0.20657916367053986, accuracy: 0.9259999990463257\n",
      "Iteration 4400 cost: 0.20740792155265808, accuracy: 0.9380000233650208\n",
      "Iteration 4500 cost: 0.21215270459651947, accuracy: 0.9480000138282776\n",
      "Iteration 4600 cost: 0.23501360416412354, accuracy: 0.9350000023841858\n",
      "Iteration 4700 cost: 0.23854662477970123, accuracy: 0.9390000104904175\n",
      "Iteration 4800 cost: 0.23814016580581665, accuracy: 0.9409999847412109\n",
      "Iteration 4900 cost: 0.20121809840202332, accuracy: 0.9290000200271606\n",
      "Iteration 5000 cost: 0.21303662657737732, accuracy: 0.9430000185966492\n",
      "CPU times: user 24.7 s, sys: 4.18 s, total: 28.8 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(iterations=5000, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
