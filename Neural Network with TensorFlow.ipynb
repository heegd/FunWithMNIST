{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "A neural network is like logistic regression in the sense that first a linear function is calculated and then it is run through an activiation function.  However, in a neural network, this linear/activation is chained together multiple times, with the result of the previous activation function feeding into the next linear function.  Each linear/activation pair is a layer in the neural network.\n",
    "\n",
    "\n",
    "Again, the linear and activtion functions are as follows:  \n",
    "A linear function is first calculated.  \n",
    "**z = xw + b**  \n",
    "Where **w** is a tensor of weights, **x** is a tensor of features, and **b** is a tensor of biases.  \n",
    "\n",
    "\n",
    "Then the output of the linear function is run through an activation function.  \n",
    "**a = g(z)**  \n",
    "Where **g()** is the activation function.\n",
    "\n",
    "\n",
    "For this simple example we are assuming the ReLU activation function for the activation function of the inner layers.\n",
    "\n",
    "\n",
    "All we have to chain tensors together that perform these two calculations for as many layers as the user wants.  The user should also be able to specify how many outputs each layer has.  We will do this by letting them specify a list of layer sizes, where the index of the list corresponds to the layer of the network, and the value at that index corresponds to the number of outputs in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x_tensor, layer_sizes):\n",
    "    for layer, layer_size in enumerate(layer_sizes):\n",
    "        if layer == 0:\n",
    "            # This is the first layer - there is nothing to do besides set up the input for the next layer.\n",
    "            a_tensor = x_tensor\n",
    "        else:\n",
    "            # This is not the first layer - apply the linear function.\n",
    "            with tf.variable_scope('Layer{0}'.format(layer), tf.AUTO_REUSE):\n",
    "                # Initialize the weights and biases tensors for this layer.\n",
    "                w_tensor = tf.get_variable(name='w', shape=(prev_layer_size, layer_size), initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b_tensor = tf.get_variable(name='b', shape=(1, layer_size), initializer=tf.contrib.layers.xavier_initializer())\n",
    "                \n",
    "                # The linear function for this layer.\n",
    "                z_tensor = tf.matmul(a_tensor, w_tensor) + b_tensor\n",
    "                \n",
    "                if layer < len(layer_sizes) - 1:\n",
    "                    # This is not the last layer - apply the ReLU activation function.\n",
    "                    a_tensor = tf.nn.relu(z_tensor)\n",
    "            \n",
    "        prev_layer_size = layer_size\n",
    "                \n",
    "    return z_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Cost\n",
    "The measurement of how how well the parameters fit the training values during training.  The goal is the minimize this difference.\n",
    "\n",
    "\n",
    "In TensorFlow, the last activation function is built into the cost.  Here, we are using the softmax activation which gives a probability of each output class being true.  All the probabilities sum to 1 for each example.\n",
    "\n",
    "\n",
    "The cost function we are using is cross entropy, which measures the distance between the tensor of probabilities from the output of the softmax and the actual values, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cost(z_tensor, y_tensor):\n",
    "    with tf.variable_scope('CostFunction'):\n",
    "        cost_tensor = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z_tensor, labels=y_tensor))\n",
    "        \n",
    "    return cost_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimizer\n",
    "Chose which optimization algorithm to use.  This is the algorithm that adjusts the weights and biases each execution to bring the cost down.  TensorFlow comes with a good selection of pre-built optimizers.  We'll use Adam here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(cost_tensor, learning_rate):\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer_tensor = tf.train.AdamOptimizer(learning_rate).minimize(cost_tensor)\n",
    "        \n",
    "    return optimizer_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Accuracy Measurement\n",
    "The measurement of accurate the model is at predicting outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_accuracy(y_hat_tensor, y_tensor):\n",
    "    with tf.variable_scope('AccuracyFunction'):\n",
    "        # A bool tensor of where the predictions matched the labels.\n",
    "        correct_predictions_tensor = tf.equal(tf.argmax(y_hat_tensor, axis=1), tf.argmax(y_tensor, axis=1))\n",
    "        # Convert the true/false values into 0 or 1.\n",
    "        correct_predictions_tensor = tf.cast(correct_predictions_tensor, tf.float32)\n",
    "        # The mean of the correct_preditions_tensor will now give us the accuracy.\n",
    "        accuracy_tensor = tf.reduce_mean(correct_predictions_tensor)\n",
    "        \n",
    "    return accuracy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "Put the pieces together to build the model, input features, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(layer_sizes, learning_rate=0.0001, iterations=1000, batch_size=100):\n",
    "    ops.reset_default_graph()\n",
    "    \n",
    "    x_tensor = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_tensor = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    z_tensor = build_model(x_tensor, layer_sizes)\n",
    "    cost_tensor = build_cost(z_tensor, y_tensor)\n",
    "    optimizer_tensor = build_optimizer(cost_tensor, learning_rate)\n",
    "    accuracy_tensor = build_accuracy(z_tensor, y_tensor)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(1, iterations + 1):\n",
    "            x, y = mnist_data.train.next_batch(batch_size)\n",
    "            accuracy, cost, _ = sess.run([accuracy_tensor, cost_tensor, optimizer_tensor], feed_dict={x_tensor:x, y_tensor:y})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print('Iteration {0} cost: {1}, accuracy: {2}'.format(i, cost, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!\n",
    "With a neural network, the train accuracy is in the 99's for the MNIST dataset.  Much better than logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 cost: 0.08462731540203094, accuracy: 0.9779999852180481\n",
      "Iteration 200 cost: 0.053787849843502045, accuracy: 0.9829999804496765\n",
      "Iteration 300 cost: 0.04631044343113899, accuracy: 0.9850000143051147\n",
      "Iteration 400 cost: 0.03487623482942581, accuracy: 0.9860000014305115\n",
      "Iteration 500 cost: 0.033800847828388214, accuracy: 0.9860000014305115\n",
      "Iteration 600 cost: 0.02459860034286976, accuracy: 0.9909999966621399\n",
      "Iteration 700 cost: 0.05959990993142128, accuracy: 0.9860000014305115\n",
      "Iteration 800 cost: 0.016436897218227386, accuracy: 0.9919999837875366\n",
      "Iteration 900 cost: 0.014681417495012283, accuracy: 0.9959999918937683\n",
      "Iteration 1000 cost: 0.010377823375165462, accuracy: 0.9980000257492065\n",
      "Iteration 1100 cost: 0.0393538735806942, accuracy: 0.9900000095367432\n",
      "Iteration 1200 cost: 0.036680907011032104, accuracy: 0.9890000224113464\n",
      "Iteration 1300 cost: 0.021307144314050674, accuracy: 0.9950000047683716\n",
      "Iteration 1400 cost: 0.008211578242480755, accuracy: 0.9959999918937683\n",
      "Iteration 1500 cost: 0.01594598963856697, accuracy: 0.9950000047683716\n",
      "Iteration 1600 cost: 0.0039867800660431385, accuracy: 0.9980000257492065\n",
      "Iteration 1700 cost: 0.021984629333019257, accuracy: 0.9909999966621399\n",
      "Iteration 1800 cost: 0.011560508981347084, accuracy: 0.9959999918937683\n",
      "Iteration 1900 cost: 0.030389130115509033, accuracy: 0.9919999837875366\n",
      "Iteration 2000 cost: 0.019878366962075233, accuracy: 0.9950000047683716\n",
      "Iteration 2100 cost: 0.003901147283613682, accuracy: 0.9990000128746033\n",
      "Iteration 2200 cost: 0.00971194077283144, accuracy: 0.996999979019165\n",
      "Iteration 2300 cost: 0.017874371260404587, accuracy: 0.996999979019165\n",
      "Iteration 2400 cost: 0.05504543334245682, accuracy: 0.9860000014305115\n",
      "Iteration 2500 cost: 0.009690893813967705, accuracy: 0.996999979019165\n",
      "Iteration 2600 cost: 0.0018248595297336578, accuracy: 0.9990000128746033\n",
      "Iteration 2700 cost: 0.02575286477804184, accuracy: 0.9919999837875366\n",
      "Iteration 2800 cost: 0.02152998559176922, accuracy: 0.9940000176429749\n",
      "Iteration 2900 cost: 0.028523793444037437, accuracy: 0.9950000047683716\n",
      "Iteration 3000 cost: 0.014382714405655861, accuracy: 0.9950000047683716\n",
      "Iteration 3100 cost: 0.009147570468485355, accuracy: 0.996999979019165\n",
      "Iteration 3200 cost: 0.009279168210923672, accuracy: 0.9980000257492065\n",
      "Iteration 3300 cost: 0.0084858862683177, accuracy: 0.9950000047683716\n",
      "Iteration 3400 cost: 0.0028648735024034977, accuracy: 0.9980000257492065\n",
      "Iteration 3500 cost: 0.016067933291196823, accuracy: 0.9929999709129333\n",
      "Iteration 3600 cost: 0.0046453094109892845, accuracy: 0.996999979019165\n",
      "Iteration 3700 cost: 0.0034760634880512953, accuracy: 0.9990000128746033\n",
      "Iteration 3800 cost: 0.007261574734002352, accuracy: 0.996999979019165\n",
      "Iteration 3900 cost: 0.0023204456083476543, accuracy: 0.9990000128746033\n",
      "Iteration 4000 cost: 0.007331552915275097, accuracy: 0.9959999918937683\n",
      "Iteration 4100 cost: 0.0019510535057634115, accuracy: 1.0\n",
      "Iteration 4200 cost: 0.02043573558330536, accuracy: 0.9959999918937683\n",
      "Iteration 4300 cost: 0.01850321888923645, accuracy: 0.9959999918937683\n",
      "Iteration 4400 cost: 0.056535687297582626, accuracy: 0.9940000176429749\n",
      "Iteration 4500 cost: 0.0052621676586568356, accuracy: 0.9980000257492065\n",
      "Iteration 4600 cost: 0.005402721464633942, accuracy: 0.9990000128746033\n",
      "Iteration 4700 cost: 0.012462098151445389, accuracy: 0.9980000257492065\n",
      "Iteration 4800 cost: 0.0021571333054453135, accuracy: 1.0\n",
      "Iteration 4900 cost: 0.009712062776088715, accuracy: 0.9990000128746033\n",
      "Iteration 5000 cost: 0.003111358731985092, accuracy: 0.9990000128746033\n",
      "CPU times: user 30.5 s, sys: 4.79 s, total: 35.3 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "layer_sizes = [784, 100, 500, 100, 10]\n",
    "\n",
    "train(layer_sizes, learning_rate = 0.01, iterations=5000, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
